# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0.  If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.
#
# Copyright 1997 - July 2008 CWI, August 2008 - 2016 MonetDB B.V.

import logging

from pymonetdb.sql import monetize, pythonize
from pymonetdb.exceptions import ProgrammingError, InterfaceError
from pymonetdb import mapi
from six import u, PY2

from enum import Enum

class BinaryTypes(Enum):
    int8 = 1
    int16 = 2
    int32 = 3
    int64 = 4
    int128 = 5
    float32 = 6
    float64 = 7

logger = logging.getLogger("pymonetdb")

class Cursor(object):
    """This object represents a database cursor, which is used to manage
    the context of a fetch operation. Cursors created from the same
    connection are not isolated, i.e., any changes done to the
    database by a cursor are immediately visible by the other
    cursors"""
    def __init__(self, connection):
        """This read-only attribute return a reference to the Connection
        object on which the cursor was created."""
        self.connection = connection

        # last executed operation (query)
        self.operation = ""

        # This read/write attribute specifies the number of rows to
        # fetch at a time with .fetchmany()
        self.arraysize = connection.replysize

        # This read-only attribute specifies the number of rows that
        # the last .execute*() produced (for DQL statements like
        # 'select') or affected (for DML statements like 'update' or
        # 'insert').
        #
        # The attribute is -1 in case no .execute*() has been
        # performed on the cursor or the rowcount of the last
        # operation is cannot be determined by the interface.
        self.rowcount = -1

        # This read-only attribute is a sequence of 7-item
        # sequences.
        #
        # Each of these sequences contains information describing
        # one result column:
        #
        #   (name,
        #    type_code,
        #    display_size,
        #    internal_size,
        #    precision,
        #    scale,
        #    null_ok)
        #
        # This attribute will be None for operations that
        # do not return rows or if the cursor has not had an
        # operation invoked via the .execute*() method yet.
        self.description = None

        self.null_values = None

        # This read-only attribute indicates at which row
        # we currently are
        self.rownumber = -1

        self.__executed = None

        # the offset of the current resultset in the total resultset
        self.__offset = 0

        # the resultset
        self.__rows = []

        # used to identify a query during server contact.
        # Only select queries have query ID
        self.__query_id = -1

        # This is a Python list object to which the interface appends
        # tuples (exception class, exception value) for all messages
        # which the interfaces receives from the underlying database for
        # this cursor.
        #
        # The list is cleared by all standard cursor methods calls (prior
        # to executing the call) except for the .fetch*() calls
        # automatically to avoid excessive memory usage and can also be
        # cleared by executing "del cursor.messages[:]".
        #
        # All error and warning messages generated by the database are
        # placed into this list, so checking the list allows the user to
        # verify correct operation of the method calls.
        self.messages = []

        # This read-only attribute provides the rowid of the last
        # modified row (most databases return a rowid only when a single
        # INSERT operation is performed). If the operation does not set
        # a rowid or if the database does not support rowids, this
        # attribute should be set to None.
        #
        # The semantics of .lastrowid are undefined in case the last
        # executed statement modified more than one row, e.g. when
        # using INSERT with .executemany().
        self.lastrowid = None

    def __check_executed(self):
        if not self.__executed:
            self.__exception_handler(ProgrammingError, "do a execute() first")

    def close(self):
        """ Close the cursor now (rather than whenever __del__ is
        called).  The cursor will be unusable from this point
        forward; an Error (or subclass) exception will be raised
        if any operation is attempted with the cursor."""
        self.connection = None

    def execute(self, operation, parameters=None):
        """Prepare and execute a database operation (query or
        command).  Parameters may be provided as mapping and
        will be bound to variables in the operation.
        """

        if not self.connection:
            self.__exception_handler(ProgrammingError, "cursor is closed")

        # clear message history
        self.messages = []

        # convert to utf-8
        if PY2:
            if type(operation) == unicode:
                # don't decode if it is already unicode
                operation = operation.encode('utf-8')
            else:
                operation = u(operation).encode('utf-8')

        # set the number of rows to fetch
        if self.arraysize != self.connection.replysize:
            self.connection.set_replysize(self.arraysize)

        if operation == self.operation:
            # same operation, DBAPI mentioned something about reuse
            # but monetdb doesn't support this
            pass
        else:
            self.operation = operation

        query = ""
        if parameters:
            if isinstance(parameters, dict):
                query = operation % dict([(k, monetize.convert(v))
                                         for (k, v) in parameters.items()])
            elif type(parameters) == list or type(parameters) == tuple:
                query = operation % tuple([monetize.convert(item) for item
                                           in parameters])
            elif isinstance(parameters, str):
                query = operation % monetize.convert(parameters)
            else:
                msg = "Parameters should be None, dict or list, now it is %s"
                self.__exception_handler(ValueError, msg % type(parameters))
        else:
            query = operation

        block = self.connection.execute(query)
        while self.__store_result(block) != None:
            block = self.connection.mapi.read_response()
        self.rownumber = 0
        self.__executed = operation
        return self.rowcount

    def executemany(self, operation, seq_of_parameters):
        """Prepare a database operation (query or command) and then
        execute it against all parameter sequences or mappings
        found in the sequence seq_of_parameters.

        It will return the number or rows affected
        """

        count = 0
        for parameters in seq_of_parameters:
            count += self.execute(operation, parameters)
        self.rowcount = count
        return count

    def fetchone(self):
        """Fetch the next row of a query result set, returning a
        single sequence, or None when no more data is available."""

        self.__check_executed()

        if self.__query_id == -1:
            msg = "query didn't result in a resultset"
            self.__exception_handler(ProgrammingError, msg)

        if self.rownumber >= self.rowcount:
            return None

        if self.rownumber >= (self.__offset + len(self.__rows)):
            self.nextset()

        result = self.__rows[self.rownumber - self.__offset]
        self.rownumber += 1
        return result

    def fetchmany(self, size=None):
        """Fetch the next set of rows of a query result, returning a
        sequence of sequences (e.g. a list of tuples). An empty
        sequence is returned when no more rows are available.

        The number of rows to fetch per call is specified by the
        parameter.  If it is not given, the cursor's arraysize
        determines the number of rows to be fetched. The method
        should try to fetch as many rows as indicated by the size
        parameter. If this is not possible due to the specified
        number of rows not being available, fewer rows may be
        returned.

        An Error (or subclass) exception is raised if the previous
        call to .execute*() did not produce any result set or no
        call was issued yet.

        Note there are performance considerations involved with
        the size parameter.  For optimal performance, it is
        usually best to use the arraysize attribute.  If the size
        parameter is used, then it is best for it to retain the
        same value from one .fetchmany() call to the next."""

        self.__check_executed()

        if self.rownumber >= self.rowcount:
            return []

        end = self.rownumber + (size or self.arraysize)
        end = min(end, self.rowcount)
        result = self.__rows[self.rownumber -
                             self.__offset:end - self.__offset]
        self.rownumber = min(end, len(self.__rows) + self.__offset)

        while (end > self.rownumber) and self.nextset():
                result += self.__rows[self.rownumber -
                                      self.__offset:end - self.__offset]
                self.rownumber = min(end, len(self.__rows) + self.__offset)
        return result

    def fetchall(self):
        """Fetch all (remaining) rows of a query result, returning
        them as a sequence of sequences (e.g. a list of tuples).
        Note that the cursor's arraysize attribute can affect the
        performance of this operation.

        An Error (or subclass) exception is raised if the previous
        call to .execute*() did not produce any result set or no
        call was issued yet."""

        self.__check_executed()

        if self.__query_id == -1:
            msg = "query didn't result in a resultset"
            self.__exception_handler(ProgrammingError, msg)

        result = self.__rows[self.rownumber - self.__offset:]
        self.rownumber = len(self.__rows) + self.__offset

        # slide the window over the resultset
        while self.nextset():
            result += self.__rows
            self.rownumber = len(self.__rows) + self.__offset

        return result

    def nextset(self):
        """This method will make the cursor skip to the next
        available set, discarding any remaining rows from the
        current set.

        If there are no more sets, the method returns
        None. Otherwise, it returns a true value and subsequent
        calls to the fetch methods will return rows from the next
        result set.

        An Error (or subclass) exception is raised if the previous
        call to .execute*() did not produce any result set or no
        call was issued yet."""

        self.__check_executed()

        if self.rownumber >= self.rowcount:
            return False

        self.__offset += len(self.__rows)

        end = min(self.rowcount, self.rownumber + self.arraysize)
        amount = end - self.__offset

        command = 'Xexport %s %s %s' % (self.__query_id, self.__offset, amount)
        block = self.connection.command(command)

        while self.__store_result(block) != None:
            block = self.connection.mapi.read_response()
        return True

    def setinputsizes(self, sizes):
        """
        This method would be used before the .execute*() method
        is invoked to reserve memory. This implementation doesn't
        use this.
        """
        pass

    def setoutputsize(self, size, column=None):
        """
        Set a column buffer size for fetches of large columns
        This implementation doesn't use this
        """
        pass

    def __iter__(self):
        return self

    def next(self):
        row = self.fetchone()
        if not row:
            raise StopIteration
        return row

    def __next__(self):
        return self.next()

    def __store_result(self, block):
        import struct
        """ parses the mapi result into a resultset"""

        if not block:
            block = ""

        column_name = ""
        scale = display_size = internal_size = precision = 0
        null_ok = False
        type_ = []

        if block.startswith(mapi.MSG_NEW_RESULT_HEADER): # *\n = new result set header
            # isolate the header
            header = block[2:]
            # unpack the header
            position = 0
            (table_id, rows, columns, self.timezone) = struct.unpack("<iqqi", header[position:position + 24])
            position += 24


            column_name = [None] * columns
            type_ = [None] * columns
            display_size = [None] * columns
            internal_size = [None] * columns
            precision = [None] * columns
            scale = [None] * columns
            null_ok = [None] * columns
            null_value = [None] * columns

            for col in xrange(columns):
                # read (tablename, columnname, typename) as null-terminated strings
                text = []
                for i in xrange(position, len(header)):
                    if header[i] == '\0':
                        text.append(header[position:i])
                        position = i + 1
                        if len(text) == 3:
                            break
                column_name[col] = text[1]
                type_[col] = text[2]
                (internal_size[col], precision[col], scale[col], null_length) = struct.unpack("<iiii", header[position:position + 16])
                position += 16
                # read the null-value
                # if null_length == 0 then the column has no NULL values
                # thus we don't need to check
                if null_length == 0:
                    null_value[col] = None
                else:
                    null_value[col] = header[position:position + null_length]
                    position += null_length
                # skip print width, not necessary
                position += 8

            self.__query_id = 1 #FIXME, also transfer query id
            self.rowcount = int(rows)
            self.__rows = []

            self.description = list(zip(column_name, type_, display_size,
                                        internal_size, precision, scale,
                                        null_ok))
            self.null_values = null_value

            self.__offset = 0
            self.lastrowid = None
            # consume the explicit flush we perform after the header
            return True
        # +\n = prot10 result set chunk, -\n = prot10 final result set chunk
        if block.startswith(mapi.MSG_NEW_RESULT_CHUNK) or block.startswith(mapi.MSG_NEW_RESULT_FINAL_CHUNK):
            # chunk message, skip the +\n (first two characters)
            # and read the row count of this message
            if self.description == None:
                self.__exception_handler(InterfaceError, "Unexpected result set chunk.")
            (rows_in_chunk,) = struct.unpack("<q", block[2:10])
            if rows_in_chunk < 0:
                # if the row count is negative this is a buffer extension message
                # we don't use a fixed size buffer so we can ignore it
                return
            # start reading the columns using the description we got from the header
            position = 10
            column_data = []
            for c in xrange(len(self.description)):
                column = self.description[c]
                # the column data start position is always 8-byte aligned (for solaris)
                # so we align 'position' to the next-highest multiple of 8
                # (or leave it unchanged if it is already a multiple of 8)
                position = position if position % 8 == 0 else position / 8 * 8 + 8
                type_ = column[1]
                typelen_ = column[3]
                precision_ = float(column[4])
                scale_ = float(column[5])
                null_value = self.null_values[c]

                def read_array_from_buffer(buffer, type_, rows_in_chunk, position, null_value):
                    # otherwise we use struct.unpack to read the binary data
                    bytes_per_value = 0
                    if type_ == BinaryTypes.int8:
                        type_fmt = "b"
                        bytes_per_value = 1
                    elif type_ == BinaryTypes.int16:
                        type_fmt = "h"
                        bytes_per_value = 2
                    elif type_ == BinaryTypes.int32:
                        type_fmt = "i"
                        bytes_per_value = 4
                    elif type_ == BinaryTypes.int64:
                        type_fmt = "q"
                        bytes_per_value = 8
                    elif type_ == BinaryTypes.float32:
                        type_fmt = "f"
                        bytes_per_value = 4
                    elif type_ == BinaryTypes.float64:
                        type_fmt = "d"
                        bytes_per_value = 8
                    else:
                        self.__exception_handler(InterfaceError, "Unexpected type, expected a BinaryTypes enum value")
                    fmtstr = "<%d%s" % (rows_in_chunk, type_fmt)
                    arr = struct.unpack(fmtstr, buffer[position:position + bytes_per_value * rows_in_chunk])
                    if null_value != None:
                        (null_value,) = struct.unpack("<%s" % type_fmt, null_value)
                        arr = [None if x == null_value else x for x in arr]
                    return arr

                if type_ == "blob":
                    # blobs start with the total length of the column as lng
                    (total_length,) = struct.unpack("<q", block[position:position + 8])
                    position += 8
                    expected_end = position + total_length
                    arr = []
                    for i in xrange(rows_in_chunk):
                        # each blob starts with a lng indicating the length of the blob
                        # followed by the actual data
                        (blob_length,) = struct.unpack("<q", block[position:position + 8])
                        position += 8
                        if blob_length < 0:
                            # blob length < 0 means NULL value
                            arr.append(None)
                        else:
                            # actual value, parse to byte array
                            bytearray_ = bytearray(block[position:position + blob_length])
                            # convert to string of bytes (this probably shouldn't happen)
                            result_str = ""
                            for x in bytearray_:
                                result_str += hex(x)[2:] if x > 0xF else '0' + hex(x)[2:]
                            arr.append(result_str.upper())
                            position += blob_length
                    if position != expected_end:
                        self.__exception_handler(InterfaceError, "Expected end position does not match up with actual end position")
                    column_data.append(arr)
                elif type_ == "tinyint":
                    column_data.append(read_array_from_buffer(block, BinaryTypes.int8, rows_in_chunk, position, null_value))
                elif type_ == "smallint":
                    column_data.append(read_array_from_buffer(block, BinaryTypes.int16, rows_in_chunk, position, null_value))
                elif type_ == "int":
                    column_data.append(read_array_from_buffer(block, BinaryTypes.int32, rows_in_chunk, position, null_value))
                elif type_ == "boolean":
                    column_data.append([(False if x == 0 else True) if x != None else None for x in read_array_from_buffer(block, BinaryTypes.int8, rows_in_chunk, position, null_value)])
                elif type_ == "double":
                    column_data.append(read_array_from_buffer(block, BinaryTypes.float64, rows_in_chunk, position, null_value))
                elif type_ == "bigint":
                    column_data.append(read_array_from_buffer(block, BinaryTypes.int64, rows_in_chunk, position, null_value))
                elif type_ == "oid":
                    if typelen_ == 8 or typelen_ == 4:
                        column_data.append(read_array_from_buffer(block, BinaryTypes.int64 if typelen_ == 8 else BinaryTypes.int32, grows_in_chunk, position, null_value))
                    else:
                        self.__exception_handler(InterfaceError, "Unexpected OID length (%d), expected (4) or (8)." % typelen_)
                elif type_ == "real":
                    column_data.append(read_array_from_buffer(block, BinaryTypes.float32, rows_in_chunk, position, null_value))
                elif type_ == "hugeint":
                    column_data.append(read_array_from_buffer(block, BinaryTypes.int128, rows_in_chunk, position, null_value))
                elif type_ == "decimal":
                    import math
                    # decimals are transferred as either (1), (2), (4), (8) or (16) byte integers
                    # the type can be determined frmo the type length
                    tpe = None
                    if typelen_ == 1:
                        tpe = BinaryTypes.int8
                    elif typelen_ == 2:
                        tpe = BinaryTypes.int16
                    elif typelen_ == 4:
                        tpe = BinaryTypes.int32
                    elif typelen_ == 8:
                        tpe = BinaryTypes.int64
                    elif typelen_ == 16:
                        tpe = BinaryTypes.int128
                    else:
                        self.__exception_handler(InterfaceError, "Unexpected decimal typelength %s." % typelen_)
                    intermediate_arr = read_array_from_buffer(block, tpe, rows_in_chunk, position, null_value)
                    # the divisor is 10^scale (i.e. scale is the position of the decimal point from the right)
                    divider = float(math.pow(10, scale_))
                    arr = [x / divider if x != None else x for x in intermediate_arr]
                    column_data.append(arr)
                elif type_ == "time" or type_ == "timetz":
                    # time is transferred as 4-byte integers 
                    # which indicate the amount of milliseconds since 00:00
                    timezone = 0
                    if type_ == "timetz":
                        timezone = self.timezone
                    import datetime
                    intermediate_arr = read_array_from_buffer(block, BinaryTypes.int32, rows_in_chunk, position, null_value)
                    # time does not support timedelta math, so we construct a datetime object and convert it back to time()
                    arr = [(datetime.datetime(1, 1, 1, 0, 0, 0) + datetime.timedelta(microseconds=(x + timezone) * 1000)).time() if x != None else x for x in intermediate_arr]
                    column_data.append(arr)
                elif type_ == "timestamp" or type_ == "timestamptz":
                    # timestamps are transferred as 8-byte unix timestamps
                    import datetime
                    timezone = 0
                    if type_ == "timestamptz":
                        timezone = self.timezone
                    intermediate_arr = read_array_from_buffer(block, BinaryTypes.int64, rows_in_chunk, position, null_value)
                    arr = [datetime.datetime.utcfromtimestamp((x + timezone) / 1000) if x != None else x for x in intermediate_arr]
                    column_data.append(arr)
                elif type_ == "varchar" or type_ == "clob" or typelen_ < 0:
                    if typelen_ >= 0:
                        # small strings might be send as fixed length strings
                        # this means every string is transferred using a fixed length typelen_
                        # shorter strings will be null-terminated, but still occupy typelen_ bytes in the message
                        arr = []
                        current_position = position
                        for i in xrange(rows_in_chunk):
                            arr.append(block[current_position:current_position + typelen_].split('\x00', 2)[0])
                            current_position += typelen_
                        column_data.append(arr)
                    else:
                        # null terminated strings
                        # this column starts with a lng that indicates the length of the column
                        (total_length,) = struct.unpack("<q", block[position:position + 8])
                        position += 8
                        # get the strings, and split them by null terminator
                        arr = block[position:position + total_length].split('\x00')[:-1]
                        if len(arr) != rows_in_chunk:
                            self.__exception_handler(InterfaceError, "Expected more strings.")
                        position += total_length
                        if null_value != None:
                            arr = [None if x == str(null_value)[0] else x.decode('utf-8') for x in arr]
                        if type_ == "date":
                            # dates are transferred as strings because date math is not fun
                            # we convert them to Python date objects here because that is what Python people expected
                            import datetime
                            arr = [datetime.datetime.strptime(x, "%Y-%m-%d").date() if x != None else x for x in arr]
                        column_data.append(arr)
                else:
                    self.__exception_handler(InterfaceError, "Unsupported type %s." % type_)
                if typelen_ > 0:
                    position += rows_in_chunk * typelen_

            self.__rows.extend(list(map(tuple, zip(*column_data))))
            # consume the explicit flush we perform after every chunk
            return True

        for line in block.split("\n"):
            if line.startswith(mapi.MSG_INFO):
                logger.info(line[1:])
                self.messages.append((Warning, line[1:]))

            elif line.startswith(mapi.MSG_QTABLE):
                (self.__query_id, rowcount, columns, tuples) = line[2:].split()[:4]

                columns = int(columns)   # number of columns in result
                self.rowcount = int(rowcount)  # total number of rows
                # tuples = int(tuples)     # number of rows in this set
                self.__rows = []

                # set up fields for description
                # table_name = [None] * columns
                column_name = [None] * columns
                type_ = [None] * columns
                display_size = [None] * columns
                internal_size = [None] * columns
                precision = [None] * columns
                scale = [None] * columns
                null_ok = [None] * columns
                # typesizes = [(0, 0)] * columns
                self.__offset = 0
                self.lastrowid = None

            elif line.startswith(mapi.MSG_HEADER):
                (data, identity) = line[1:].split("#")
                values = [x.strip() for x in data.split(",")]
                identity = identity.strip()

                if identity == "name":
                    column_name = values
                # elif identity == "table_name":
                #    table_name = values
                elif identity == "type":
                    type_ = values
                # elif identity == "length":
                #   length = values
                elif identity == "typesizes":
                    typesizes = [[int(j) for j in i.split()] for i in values]
                    internal_size = [x[0] for x in typesizes]
                    for num, typeelem in enumerate(type_):
                        if typeelem in ['decimal']:
                            precision[num] = typesizes[num][0]
                            scale[num] = typesizes[num][1]
                # else:
                #    msg = "unknown header field"
                #    self.messages.append((InterfaceError, msg))
                #    self.__exception_handler(InterfaceError, msg)

                self.description = list(zip(column_name, type_, display_size,
                                            internal_size, precision, scale,
                                            null_ok))
                self.__offset = 0
                self.lastrowid = None

            elif line.startswith(mapi.MSG_TUPLE):
                values = self.__parse_tuple(line)
                self.__rows.append(values)

            elif line.startswith(mapi.MSG_TUPLE_NOSLICE):
                self.__rows.append((line[1:],))

            elif line.startswith(mapi.MSG_QBLOCK):
                self.__rows = []

            elif line.startswith(mapi.MSG_QSCHEMA):
                self.__offset = 0
                self.lastrowid = None
                self.__rows = []
                self.description = None
                self.rowcount = -1

            elif line.startswith(mapi.MSG_QUPDATE):
                (affected, identity) = line[2:].split()[:2]
                self.__offset = 0
                self.__rows = []
                self.description = None
                self.rowcount = int(affected)
                self.lastrowid = int(identity)
                self.__query_id = -1

            elif line.startswith(mapi.MSG_QTRANS):
                self.__offset = 0
                self.lastrowid = None
                self.__rows = []
                self.description = None
                self.rowcount = -1

            elif line == mapi.MSG_PROMPT:
                return

            elif line.startswith(mapi.MSG_ERROR):
                self.__exception_handler(ProgrammingError, line[1:])

        self.__exception_handler(InterfaceError, "Unknown state, %s" % block)

    def __parse_tuple(self, line):
        """ parses a mapi data tuple, and returns a list of python types"""
        elements = line[1:-1].split(',\t')
        if len(elements) == len(self.description):
            return tuple([pythonize.convert(element.strip(), description[1])
                          for (element, description) in zip(elements,
                                                            self.description)])
        else:
            self.__exception_handler(InterfaceError,
                                     "length of row doesn't match header")

    def scroll(self, value, mode='relative'):
        """Scroll the cursor in the result set to a new position according
        to mode.

        If mode is 'relative' (default), value is taken as offset to
        the current position in the result set, if set to 'absolute',
        value states an absolute target position.

        An IndexError is raised in case a scroll operation would
        leave the result set.
        """
        self.__check_executed()

        if mode not in ['relative', 'absolute']:
            msg = "unknown mode '%s'" % mode
            self.__exception_handler(ProgrammingError, msg)

        if mode == 'relative':
            value += self.rownumber

        if value > self.rowcount:
            self.__exception_handler(IndexError,
                                     "value beyond length of resultset")

        self.__offset = value
        end = min(self.rowcount, self.rownumber + self.arraysize)
        amount = end - self.__offset
        command = 'Xexport %s %s %s' % (self.__query_id, self.__offset, amount)
        block = self.connection.command(command)
        while self.__store_result(block) != None:
            block = self.connection.mapi.read_response()

    def __exception_handler(self, exception_class, message):
        """ raises the exception specified by exception, and add the error
        to the message list """
        self.messages.append((exception_class, message))
        raise exception_class(message)
